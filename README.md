This submission consists of two programs which implement features from each of the two questions in the coding challenge.

Java (version "1.8.0_25") was used exclusively to implement both features with the Mac OS X operating system. Testing was conducted using a 2014 MacBook Air with 4GB memory and an Intel 1.4 GHz Core i5 processor. No other libraries, environments, or dependencies are required. 

The structure and content of this file folder adheres strictly to the guidelines set forth in the problem description. To execute each of the source files, use run.sh. The shell script will grab the path to its directory no matter where its located so it can execute the source files regardless of the current directory. Because the directory structure from the prompt was duplicated, all subfolder, input file, and output file names are hardcoded into the source files. The only variable argument passed by run.sh to the java programs is the directory path. Test inputs and outputs are included; they are duplicates of those present in the example repo.

words_tweeted.java reads data from tweet_input/tweets.txt and outputs an ASCII sorted list of words and frequencies of occurence to tweet_output/ft1.txt. Because this feature depends heavily on I/O, if the batch import size is greater than the size of the file, data is read in via BufferedInputStream into a single byte array, copied to a single String object via UTF-8, and then parsed. If the batch import size is less than the size of the file then the BufferedInputStream will read in chunks of the file equal to the size of the batch import size or less. If you are getting "OutOfMemoryError" and it is impossible to increase the JVM max heap size any further, decreasing the batch import size (via importSizeLimit variable) will help keep the memory load under the JVM's limit. Implementing BufferedInputStream in this way means the program is no longer I/O-bound, rather it's CPU-bound. As such, A multithreaded structure is used to parse the String object and insert words into a concurrentHashMap. I suggest you set the number of threads (the threadNum variable) equal to N + 1 threads, where N is the number of computer processors, for optimal performance. The HashMap key set is then merge sorted by ASCII and printed to tweet_output/ft1.txt. Processing a 150 MB file took less than 4 seconds.

median_unique.java reads data from tweet_input/tweets.txt and prints out the running median unique word count to tweet_output/ft2.txt. Just like words_tweeted.java, this program reads in the entire tweets.txt file (or sections of the file depending on its size in relation to the import size limit) into a byte array, copies the array to a String object, and parses the String. Again, If you are getting "OutOfMemoryError" please lower the batch import size limit (via importSizeLimit variable) to help the memory load stay under the JVM's limit. However, due to the data structure implemented and the inherent unpredictability of threads, it was impossible to ensure the quick, accurate execution of the running median with multithreading. Instead, an array of longs was used to represent a unique word count histogram. This strategy offered O(1) median return time in addition to O(1) insertion time and low, fixed data structure memory usage, far better than a typical min/max heap implementation. Processing a 150 MB file took about 4.5 seconds (outputting to ft2.txt in real time after every tweet means this is unavoidably I/O bound).

Please refer to NOTES in the source files for specific information on design decision logic, trade-offs, and clarifications.
