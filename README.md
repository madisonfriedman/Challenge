This submission consists of two programs which implement features from each of the two questions in the coding challenge.

Java (version "1.8.0_25") was used exclusively to implement both features with the Mac OS X operating system. Testing was conducted using a 2014 MacBook Air with 4GB memory and an Intel 1.4 GHz Core i5 processor. No other libraries, environments, or dependencies are required. 

The structure and content of this file folder adheres strictly to the guidelines set forth in the problem description. To execute each of the source files, use run.sh. The shell script will grab the path to its directory no matter where its located so it can execute the source files regardless of the current directory. Because the directory structure from the prompt was duplicated, all subfolder, input file, and output file names are hardcoded into the source files. The only variable argument passed by run.sh to the java programs is the directory path. Test inputs and outputs are included; they are duplicates of those present in the example repo.

words_tweeted.java reads data from tweet_input/tweets.txt and outputs an ASCII sorted list of words and frequencies of occurence to tweet_output/ft1.txt. Because this feature depends heavily on I/O, data is read in via BufferedInputStream into a single byte array, copied to a single String object via UTF-8, and then parsed. 

It is crucial that the JVM has a max allocated heap size equal to at least 2.5x the size of tweets.txt in order to be able to safely store the objects. Note that I would have implemented a method to read in and process predefined segments of the text with a variable limit (perhaps 1GB) but unfortunately doing so is impossible without a custom implementation of a method to skip bytes in the input stream. InputStream.skip(long) does NOT offer a robust implementation: <http://docs.oracle.com/javase/7/docs/api/java/io/InputStream.html#skip(long)>. I proceeded with this implementation regardless because most systems have abundant memory, or the ability to create it, and as such, most text files should be easily read.

On the plus side, implementing BufferedInputStream in this way means the program is no longer I/O-bound, rather it's CPU-bound. As such, A multithreaded structure is used to parse the String object and insert words into a concurrentHashMap. I suggest you set the number of threads (the threadNum variable) equal to N + 1 threads, where N is the number of computer processors, for optimal performance. The HashMap key set is then merge sorted by ASCII and printed to tweet_output/ft1.txt. Processing a 50 MB file took less than two seconds.

median_unique.java reads data from tweet_input/tweets.txt and prints out the running median unique word count to tweet_output/ft2.txt. Just like words_tweeted.java, this program reads in the entire tweets.txt file into a byte array, copies the array to a String object, and parses the String. However, due to the data structure implemented and the inherent unpredictability of threads, it was impossible to ensure the quick, accurate execution of the running median with multithreading. Instead, an array of longs was used to represent a unique word count histogram. This strategy offered O(1) median return time in addition to O(1) insertion time and low, fixed data structure memory usage, far better than a typical min/max heap implementation. Processing a 50 MB file took about two and a half seconds (outputting to ft2.txt in real time after every tweet means this is unavoidably I/O bound).

Please refer to NOTES in the source files for specific information on design decision logic, trade-offs, and clarifications.
